{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6058efba-4ca7-46a3-ae3b-7d1293e15fea",
   "metadata": {},
   "source": [
    "# Analyze distance to outgroup comparators\n",
    "\n",
    "Get variables from `snakemake`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a936869-3d81-455f-aed0-f53844324b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_seq_subs_file = snakemake.input.early_seq_subs\n",
    "early_seq_alignment_file = snakemake.input.early_seq_alignment\n",
    "deleted_diffs_file = snakemake.input.deleted_diffs\n",
    "deleted_alignment_file = snakemake.input.deleted_consensus\n",
    "comparator_map_file = snakemake.input.comparator_map\n",
    "region_of_interest = snakemake.params.region_of_interest\n",
    "comparators = snakemake.params.comparators\n",
    "min_frac_coverage = snakemake.params.min_frac_coverage\n",
    "samples = snakemake.params.samples\n",
    "aligners = snakemake.params.aligners\n",
    "ref_genome_name = snakemake.params.ref_genome_name\n",
    "ignore_muts_before = snakemake.params.ignore_muts_before\n",
    "ignore_muts_after = snakemake.params.ignore_muts_after\n",
    "last_date = snakemake.params.phylo_last_date\n",
    "muts_to_ignore = snakemake.params.phylo_muts_to_ignore\n",
    "collapse_rare_muts = snakemake.params.phylo_collapse_rare_muts\n",
    "filter_rare_variants = snakemake.params.phylo_filter_rare_variants\n",
    "min_frac_called = snakemake.params.phylo_min_frac_called\n",
    "cat_colors = snakemake.params.cat_colors\n",
    "subcat_colors = snakemake.params.subcat_colors\n",
    "\n",
    "alignment_all_fasta = snakemake.output.alignment_all_fasta\n",
    "alignment_all_csv = snakemake.output.alignment_all_csv\n",
    "alignment_all_no_filter_rare_fasta = snakemake.output.alignment_all_no_filter_rare_fasta\n",
    "alignment_all_no_filter_rare_csv = snakemake.output.alignment_all_no_filter_rare_csv\n",
    "deleted_csv = snakemake.output.deleted_csv\n",
    "early_seq_count_charts = snakemake.output.early_seq_counts\n",
    "early_seq_deltadist_charts = snakemake.output.early_seq_deltadist\n",
    "early_seq_deltadist_region_charts = snakemake.output.early_seq_deltadist_region\n",
    "deltadist_jitter_charts = snakemake.output.deltadist_jitter\n",
    "deleted_diffs_latex = snakemake.output.deleted_diffs_latex\n",
    "recovered_seqs_fasta = snakemake.output.recovered_seqs\n",
    "matches_in_gisaid_csv = snakemake.output.matches_in_gisaid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709fa86c-ceca-4ed2-b9da-3bd2abd64437",
   "metadata": {},
   "source": [
    "Import Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881c8b6-495b-499f-aaa0-cfd1acd1b4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "import altair_saver\n",
    "\n",
    "import Bio.SeqIO\n",
    "\n",
    "import numpy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "_ = alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af792c-7f6c-4586-81f0-1bb15fe4ff57",
   "metadata": {},
   "source": [
    "## Early GISAID sequences\n",
    "Read early sequence substitutions and comparator map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d31d5-68df-47c7-8900-e449220dba89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_seq_subs = (\n",
    "    pd.read_csv(early_seq_subs_file, na_filter=None)\n",
    "    .assign(date=lambda x: pd.to_datetime(x['date']))\n",
    "    )\n",
    "\n",
    "comparator_map = pd.read_csv(comparator_map_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06008f90-e1cb-4067-90c8-9032774d7cc0",
   "metadata": {},
   "source": [
    "Annotate whether strains from Wuhan, elsewhere in China, or outside China.\n",
    "Also make a location **sub** category that is location category or more detailed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c14873-36cd-4712-986c-2dfe60e84da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert early_seq_subs['country'].notnull().all()\n",
    "\n",
    "def location_category(row):\n",
    "    if 'Wuhan' == row['location']:\n",
    "        return 'Wuhan'\n",
    "    elif 'Wuhan' in row['strain'] and 'Beijing' not in row['strain']:\n",
    "        # there are some strains with hCoV-19/Beijing/Wuhan_IME-BJ01/2020\n",
    "        # which appear to actually be from Beijing according to most annotations\n",
    "        return 'Wuhan'\n",
    "    elif 'China' in row['country']:\n",
    "        return 'other China'\n",
    "    else:\n",
    "        return 'outside China'\n",
    "\n",
    "early_seq_subs = (\n",
    "    early_seq_subs\n",
    "    .rename(columns={'sub_category': 'location_subcategory'})\n",
    "    .assign(location_category=lambda x: x.apply(location_category, axis=1),\n",
    "            location_subcategory=lambda x: x['location_subcategory'].where(\n",
    "                                                x['location_subcategory'].astype(bool),\n",
    "                                                x['location_category'].map(lambda s: 'other Wuhan'\n",
    "                                                                                      if s == 'Wuhan' else s))\n",
    "            )\n",
    "    )\n",
    "\n",
    "(early_seq_subs\n",
    " .groupby(['location_category', 'location_subcategory'])\n",
    " .aggregate(n_seqs=pd.NamedAgg('strain', 'count'))\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8cbe21-d335-4b16-b326-d148a75c3d11",
   "metadata": {},
   "source": [
    "Plot number of sequences from each location as function of date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a41be-9650-4bee-b4cc-6201a57f6463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get counts per week as here, with counts being the week\n",
    "# before that date\n",
    "location_date_df = (\n",
    "    early_seq_subs\n",
    "    [['strain', 'location_category', 'date']]\n",
    "    # https://stackoverflow.com/a/45281439/4191652\n",
    "    .groupby(['location_category',\n",
    "              pd.Grouper(key='date', freq='W-FRI')],\n",
    "              )\n",
    "    .aggregate(nseqs=pd.NamedAgg('strain', 'count'))\n",
    "    .reset_index()\n",
    "    )\n",
    "\n",
    "assert set(location_date_df['location_category']) == set(cat_colors)\n",
    "# make chart\n",
    "location_date_chart = (\n",
    "    alt.Chart(location_date_df)\n",
    "    .encode(x=alt.X('date:T',\n",
    "                    axis=alt.Axis(labelAngle=-90,\n",
    "                                  values=location_date_df['date'].unique(),\n",
    "                                  format='%b %d',\n",
    "                                  tickCount=location_date_df['date'].nunique(),\n",
    "                                  ),\n",
    "                    title='week in 2019 or 2020',\n",
    "                    ),\n",
    "            y=alt.Y('nseqs',\n",
    "                    title='sequences from prior week',\n",
    "                    ),\n",
    "            color=alt.Color('location_category:N',\n",
    "                            legend=alt.Legend(title='location'),\n",
    "                            scale=alt.Scale(range=list(cat_colors.values())),\n",
    "                            sort=list(cat_colors),\n",
    "                            ),\n",
    "            tooltip=['date',\n",
    "                     alt.Tooltip('nseqs',\n",
    "                                 title='number of sequences',\n",
    "                                 ),\n",
    "                     alt.Tooltip('location_category',\n",
    "                                 title='location',\n",
    "                                 ),\n",
    "                     ],\n",
    "            )\n",
    "    .mark_line(point=True)\n",
    "    .configure_point(size=65)\n",
    "    .configure_axis(grid=False)\n",
    "    .properties(width=275,\n",
    "                height=150,\n",
    "                )\n",
    "    )\n",
    "\n",
    "for f in early_seq_count_charts:\n",
    "    print(f\"Saving to {f}\")\n",
    "    if os.path.splitext(f) == '.html':\n",
    "        location_date_chart.save(f)\n",
    "    else:\n",
    "        altair_saver.save(location_date_chart, f)\n",
    "\n",
    "location_date_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d57f3f-d245-485d-bb70-5804df0de01f",
   "metadata": {},
   "source": [
    "Define a function that computes the total change in Hamming distance from each comparator relative to the reference based on the substitutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee995e1b-5705-4209-a853-2699d2d48d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_to_ref = comparator_map.set_index('site')['reference'].to_dict()\n",
    "comparator_site_to_nt = {comparator: comparator_map.set_index('site')[comparator].to_dict()\n",
    "                         for comparator in comparators}\n",
    "\n",
    "def delta_distance_comparator(subs_str, comparator):\n",
    "    \"\"\"Total change in Hamming distance from comparator relative to reference.\"\"\"\n",
    "    site_to_nt = comparator_site_to_nt[comparator]\n",
    "    n = 0\n",
    "    for s in [s for s in subs_str.split(',') if s]:\n",
    "        m = re.fullmatch('(?P<wt>[ACGT])(?P<site>\\d+)(?P<mut>[ACGT])', s)\n",
    "        if not m:\n",
    "            raise ValueError(f\"cannot match {s}\")\n",
    "        wt = m.group('wt')\n",
    "        site = int(m.group('site'))\n",
    "        mut = m.group('mut')\n",
    "        assert site_to_ref[site] == wt\n",
    "        comp = site_to_nt[site]\n",
    "        if comp in 'ACGT':\n",
    "            if mut == comp:\n",
    "                n -= 1\n",
    "            elif mut != comp:\n",
    "                n += 1\n",
    "        elif comp not in ['-', 'N']:\n",
    "            raise ValueError(f\"invalid comparator identity {comp}\")\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3451e-973d-47af-8ed2-8ba220423cc8",
   "metadata": {},
   "source": [
    "Apply this function to each comparator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3daea0-e164-4f88-9fdf-2d461044cc98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for comparator in comparators:\n",
    "    \n",
    "    ref_dist = (comparator_map\n",
    "                .query('reference.str.fullmatch(\"[ACGT]+\")')\n",
    "                .query(f'{comparator}.str.fullmatch(\"[ACGT]+\")')\n",
    "                .query(f'reference != {comparator}')\n",
    "                .shape[0]\n",
    "                )\n",
    "    print(f\"Reference distance from {comparator} is {ref_dist}\")\n",
    "    \n",
    "    early_seq_subs[f\"{comparator}_delta_dist\"] = (early_seq_subs\n",
    "                                                  ['substitutions']\n",
    "                                                  .apply(delta_distance_comparator,\n",
    "                                                         args=(comparator,)\n",
    "                                                         )\n",
    "                                                  )\n",
    "    early_seq_subs[f\"{comparator}_dist\"] = early_seq_subs[f\"{comparator}_delta_dist\"] + ref_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b26c15-a97b-4a76-b03f-d1491f15f048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T14:50:15.529172Z",
     "iopub.status.busy": "2021-06-05T14:50:15.528973Z",
     "iopub.status.idle": "2021-06-05T14:50:15.568412Z",
     "shell.execute_reply": "2021-06-05T14:50:15.567808Z",
     "shell.execute_reply.started": "2021-06-05T14:50:15.529152Z"
    },
    "tags": []
   },
   "source": [
    "Make a tidy data frame with these distances.\n",
    "We use the actual distances, not the delta distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e66bc-2be5-4a03-9402-7667ff82997d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(early_seq_subs) == early_seq_subs['strain'].nunique()\n",
    "\n",
    "delta_dist = (\n",
    "    early_seq_subs\n",
    "    .melt(id_vars=['strain', 'gisaid_epi_isl', 'date', 'location_category', 'location_subcategory',\n",
    "                   'frac_called', 'substitutions', 'huanan_market'],\n",
    "          value_vars=[f\"{comparator}_substitutions\" for comparator in comparators],\n",
    "          var_name='outgroup',\n",
    "          value_name='substitutions_to_outgroup')\n",
    "    .assign(outgroup=lambda x: x['outgroup'].str.replace('_substitutions', ''),\n",
    "            n_substitutions=lambda x: x['substitutions'].map(lambda s: len([s for s in s.split(',') if s])),\n",
    "            )\n",
    "    .merge(early_seq_subs.melt(id_vars='strain',\n",
    "                               value_vars=[f\"{comparator}_delta_dist\" for\n",
    "                                           comparator in comparators],\n",
    "                               var_name='outgroup',\n",
    "                               value_name='delta_distance_to_outgroup',\n",
    "                               )\n",
    "                         .assign(outgroup=lambda x: x['outgroup'].str\n",
    "                                                    .replace('_delta_dist', '')\n",
    "                                 ),\n",
    "           on=['strain', 'outgroup'],\n",
    "           validate='one_to_one',\n",
    "           )\n",
    "    )\n",
    "\n",
    "assert len(delta_dist) == len(early_seq_subs) * len(comparators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e863f9-89fb-4376-81f4-f52b2fa5d9e3",
   "metadata": {},
   "source": [
    "Now do the same thing just for the region of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50687ab2-ee3e-4f6c-94b2-7b7dfdc20bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = region_of_interest['start']\n",
    "end = region_of_interest['end']\n",
    "def subs_in_region(subs_str):\n",
    "    return ','.join(s for s in subs_str.split(',')\n",
    "                    if s and start <= int(s[1 : -1]) <= end)\n",
    "    \n",
    "early_seq_subs_region = (\n",
    "    early_seq_subs\n",
    "    .assign(substitutions=lambda x: x['substitutions'].map(subs_in_region),\n",
    "            frac_called=lambda x: x['frac_called_in_region_of_interest'])\n",
    "    )\n",
    "\n",
    "for comparator in comparators:\n",
    "    early_seq_subs_region[f\"{comparator}_delta_dist\"] = (\n",
    "                                                early_seq_subs_region\n",
    "                                                ['substitutions']\n",
    "                                                .apply(delta_distance_comparator,\n",
    "                                                       args=(comparator,)\n",
    "                                                       )\n",
    "                                                )\n",
    "    early_seq_subs_region[f\"{comparator}_substitutions\"] = (\n",
    "                        early_seq_subs_region\n",
    "                        [f\"{comparator}_substitutions\"]\n",
    "                        .map(subs_in_region)\n",
    "                        )\n",
    "    \n",
    "delta_dist_region = (\n",
    "    early_seq_subs_region\n",
    "    .melt(id_vars=['strain', 'gisaid_epi_isl', 'date', 'location_category', 'location_subcategory',\n",
    "                   'frac_called', 'substitutions', 'huanan_market'],\n",
    "          value_vars=[f\"{comparator}_substitutions\" for comparator in comparators],\n",
    "          var_name='outgroup',\n",
    "          value_name='substitutions_to_outgroup')\n",
    "    .assign(outgroup=lambda x: x['outgroup'].str.replace('_substitutions', ''),\n",
    "            n_substitutions=lambda x: x['substitutions'].map(lambda s: len([s for s in s.split(',') if s])),\n",
    "            )\n",
    "    .merge(early_seq_subs_region.melt(id_vars='strain',\n",
    "                                      value_vars=[f\"{comparator}_delta_dist\" for\n",
    "                                                  comparator in comparators],\n",
    "                                      var_name='outgroup',\n",
    "                                      value_name='delta_distance_to_outgroup',\n",
    "                                      )\n",
    "                                .assign(outgroup=lambda x: x['outgroup'].str\n",
    "                                                           .replace('_delta_dist', '')\n",
    "                                        ),\n",
    "           on=['strain', 'outgroup'],\n",
    "           validate='one_to_one',\n",
    "           )\n",
    "    )\n",
    "\n",
    "assert len(delta_dist_region) == len(early_seq_subs) * len(comparators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da864f6-3e31-427f-981f-7225cefa7caf",
   "metadata": {},
   "source": [
    "Get identity at site 28144 and add to data frame of delta distances for region of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8dd8d-6f97-42b8-9386-4cd2af56b81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nt_28144 = {}\n",
    "for s in Bio.SeqIO.parse(early_seq_alignment_file, 'fasta'):\n",
    "    nt_28144[s.id] = str(s.seq[28143]).upper()\n",
    "\n",
    "delta_dist_region = (\n",
    "    delta_dist_region\n",
    "    .assign(nt_28144=lambda x: x['strain'].map(nt_28144))\n",
    "    )\n",
    "assert delta_dist_region['nt_28144'].notnull().all()\n",
    "\n",
    "display(delta_dist_region\n",
    "        [['strain', 'location_category', 'nt_28144']]\n",
    "        .drop_duplicates()\n",
    "        .groupby(['location_category', 'nt_28144'])\n",
    "        .aggregate(n_strains=pd.NamedAgg('strain', 'count'))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30694d9a-4229-4b66-9941-a33dffe6314e",
   "metadata": {},
   "source": [
    "Function to plot delta distances versus date of isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a82a07-87a7-4f37-89ad-fc5c6cc4c98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_dist_chart_height = 170\n",
    "\n",
    "outgroup_selection = alt.selection_single(\n",
    "    name='sequence',\n",
    "    fields=['outgroup'],\n",
    "    bind=alt.binding_select(options=comparators),\n",
    "    init={'outgroup': comparators[0]},\n",
    "    )\n",
    "\n",
    "jitter_slider = alt.selection_single(\n",
    "        name='y_axis_jitter',\n",
    "        fields=['amount'],\n",
    "        init={'amount': 0.2},\n",
    "        bind=alt.binding_range(min=0, max=1)\n",
    "        )\n",
    "\n",
    "def rand_jitter(n, seed):\n",
    "    \"\"\"Calculate random jitter as here:\n",
    "    https://www.geeksforgeeks.org/how-to-make-stripplot-with-jitter-in-altair-python/\n",
    "    \"\"\"\n",
    "    numpy.random.seed(seed)\n",
    "    return (numpy.sqrt(-2 * numpy.log(numpy.random.rand(n))) *\n",
    "            numpy.cos(2 * numpy.pi * numpy.random.rand(n)))\n",
    "\n",
    "def get_delta_distance_plot(df):\n",
    "    \n",
    "    assert set(df['location_category']) == set(cat_colors)\n",
    "    \n",
    "    y_extent = df['delta_distance_to_outgroup'].max() - df['delta_distance_to_outgroup'].min()\n",
    "    \n",
    "    delta_distance_points = (\n",
    "        alt.Chart(df\n",
    "                  .assign(jitter_y=y_extent / 20 * rand_jitter(len(df), seed=1),\n",
    "                          huanan_market=lambda x: x['huanan_market'].map({True: 'associated with market',\n",
    "                                                                          False: 'not associated with market'})\n",
    "                          )\n",
    "                  )\n",
    "        .encode(x=alt.X('date:T',\n",
    "                        axis=alt.Axis(labelAngle=-90),\n",
    "                        title='date in 2019 or 2020',\n",
    "                        ),\n",
    "                y=alt.Y('y:Q',\n",
    "                        title='relative mutations from outgroup',\n",
    "                        scale=alt.Scale(nice=False),\n",
    "                        axis=alt.Axis(tickMinStep=1),\n",
    "                        ),\n",
    "                color=alt.Color('location_category:N',\n",
    "                                scale=alt.Scale(range=list(cat_colors.values())),\n",
    "                                legend=None,\n",
    "                                sort=list(cat_colors),\n",
    "                                ),\n",
    "                shape=alt.Shape('huanan_market:N',\n",
    "                                legend=alt.Legend(orient='top',\n",
    "                                                  symbolFillColor=cat_colors['Wuhan'],\n",
    "                                                  symbolStrokeColor=cat_colors['Wuhan'],\n",
    "                                                  symbolStrokeWidth=0,\n",
    "                                                  offset=0,\n",
    "                                                  symbolSize=50,\n",
    "                                                  ),\n",
    "                                scale=alt.Scale(range=['square', 'circle']),\n",
    "                                title='from Huanan Seafood Market',\n",
    "                                ),\n",
    "                strokeWidth=alt.StrokeWidth('huanan_market:N',\n",
    "                                            scale=alt.Scale(range=[2, 0]),\n",
    "                                            legend=None,\n",
    "                                            title='from Huanan Seafood Market',\n",
    "                                            ),\n",
    "                tooltip=['strain',\n",
    "                         alt.Tooltip('gisaid_epi_isl',\n",
    "                                     title='GISAID ID'),\n",
    "                         'date',\n",
    "                         alt.Tooltip('n_substitutions',\n",
    "                                     title='number substitutions'),\n",
    "                         'substitutions',\n",
    "                         alt.Tooltip('substitutions_to_outgroup',\n",
    "                                     title='substitutions to outgroup'),\n",
    "                         alt.Tooltip('frac_called',\n",
    "                                     title='fraction sites called',\n",
    "                                     format='.3f'),\n",
    "                         alt.Tooltip('huanan_market:N',\n",
    "                                     title='from seafood market'),\n",
    "                         ],\n",
    "                )\n",
    "        .mark_point(filled=True,\n",
    "                    opacity=0.5,\n",
    "                    size=30,\n",
    "                    stroke='black',\n",
    "                    )\n",
    "        .transform_filter(outgroup_selection)\n",
    "        .transform_calculate(\n",
    "            y='datum.delta_distance_to_outgroup + datum.jitter_y * y_axis_jitter.amount'\n",
    "            )\n",
    "        .properties(height=delta_dist_chart_height,\n",
    "                    width=270,\n",
    "                    )\n",
    "        )\n",
    "\n",
    "    delta_distance_lines = (\n",
    "        delta_distance_points\n",
    "        .transform_regression('date', 'delta_distance_to_outgroup',\n",
    "                              groupby=['location_category'])\n",
    "        .encode(color=alt.value('#999999'),\n",
    "                y='delta_distance_to_outgroup')\n",
    "        .mark_line(opacity=0.3,\n",
    "                   size=5,\n",
    "                   point=False,\n",
    "                   )\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        (delta_distance_points + delta_distance_lines)\n",
    "        .add_selection(jitter_slider,\n",
    "                       outgroup_selection,\n",
    "                       )\n",
    "        .facet(facet=alt.Facet('location_category:N',\n",
    "                               title=None,\n",
    "                               header=alt.Header(labelFontStyle='bold',\n",
    "                                                 labelPadding=1,\n",
    "                                                 labelFontSize=12,\n",
    "                                                 ),\n",
    "                               ),\n",
    "               columns=3,\n",
    "               spacing=2,\n",
    "               )\n",
    "        .configure_axis(grid=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54799d0f-867c-46e6-b8b4-bb6f9cba1f9c",
   "metadata": {},
   "source": [
    "Make chart for whole genome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8234e-99a4-4298-8309-16254df604ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_distance_all_chart = get_delta_distance_plot(delta_dist)\n",
    "\n",
    "for f in early_seq_deltadist_charts:\n",
    "    print(f\"Saving to {f}\")\n",
    "    if os.path.splitext(f) == '.html':\n",
    "        delta_distance_all_chart.save(f)\n",
    "    else:\n",
    "        altair_saver.save(delta_distance_all_chart, f)\n",
    "\n",
    "delta_distance_all_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3b437-a32e-4b92-b5e2-1eafe8262537",
   "metadata": {},
   "source": [
    "Make chart for region of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca0af5-5618-42b5-9f9f-aa2a3ec5f31e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_distance_region_chart = get_delta_distance_plot(delta_dist_region)\n",
    "\n",
    "for f in early_seq_deltadist_region_charts:\n",
    "    print(f\"Saving to {f}\")\n",
    "    if os.path.splitext(f) == '.html':\n",
    "        delta_distance_region_chart.save(f)\n",
    "    else:\n",
    "        altair_saver.save(delta_distance_region_chart, f)\n",
    "\n",
    "delta_distance_region_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4004f0-6a83-4fda-8fd8-e8e69051d4a2",
   "metadata": {},
   "source": [
    "## Read the alignment of all early seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72df7c1-b7a6-47cc-832b-9763fe917c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_seq_alignment = list(Bio.SeqIO.parse(early_seq_alignment_file, 'fasta'))\n",
    "\n",
    "aligned_length = len(early_seq_alignment[0])\n",
    "print(f\"Alignment length is {aligned_length}\")\n",
    "\n",
    "assert all(len(s) == aligned_length for s in early_seq_alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f144d5-b406-4dc6-8280-f331487eba1d",
   "metadata": {},
   "source": [
    "## Deleted sequence set\n",
    "Get information on substitutions in deleted sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a18a01-6b72-453e-860e-c89faa128a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deleted_diffs = pd.read_csv(deleted_diffs_file)\n",
    "\n",
    "deleted_alignment = pd.read_csv(deleted_alignment_file)\n",
    "\n",
    "# make sure we have information for the expected samples / aligners\n",
    "expect_samples_aligners = {(s, a) for s, a in itertools.product(samples, aligners)}\n",
    "assert expect_samples_aligners == set(deleted_alignment[['sample', 'aligner']]\n",
    "                                      .itertuples(index=False, name=None))\n",
    "assert expect_samples_aligners.issuperset(deleted_diffs[['sample', 'aligner']]\n",
    "                                          .itertuples(index=False, name=None))\n",
    "\n",
    "# make sure alignment of correct length\n",
    "assert all(deleted_alignment['sequence'].map(len) == aligned_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d971b3-96f4-4faa-a955-a5a8356b3bfd",
   "metadata": {},
   "source": [
    "Get the region of interest sequence and fraction sites called in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9a6bb-6f11-4e30-b4b3-607a92321734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_groups = {sample: d['patient_group'] for sample, d in samples.items()}\n",
    "\n",
    "deleted_alignment = (\n",
    "    deleted_alignment\n",
    "    .assign(sequence_region=lambda x: x['sequence'].str[start - 1: end],\n",
    "            frac_called_region=lambda x: x['sequence_region']\n",
    "                                         .map(lambda s: sum(nt in 'ACGT' for nt in s) /\n",
    "                                                        (end - start + 1)\n",
    "                                              ),\n",
    "            patient_group=lambda x: x['sample'].map(patient_groups)\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b03eb-786a-46a9-9d97-3c8ee68e20ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:49.400144Z",
     "iopub.status.busy": "2021-06-07T00:10:49.399488Z",
     "iopub.status.idle": "2021-06-07T00:10:49.416614Z",
     "shell.execute_reply": "2021-06-07T00:10:49.415638Z",
     "shell.execute_reply.started": "2021-06-07T00:10:49.400071Z"
    }
   },
   "source": [
    "Get the differences from the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a6ab3-46af-4df3-9c29-a1ac0fecad00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Reading diffs from ref from {deleted_diffs_file}, subsetting to sites {start} to {end}\")\n",
    "\n",
    "deleted_diffs = (\n",
    "    pd.read_csv(deleted_diffs_file)\n",
    "    .query('(site >= @start) and (site <= @end)')\n",
    "    .assign(mutation=lambda x: x['reference'] + x['site'].astype(str) + x['consensus'],\n",
    "            mutation_str=lambda x: x['mutation'] + '(' +\n",
    "                                   x.apply(lambda r: ','.join(f\"{nt}={r[nt]}\" for nt in\n",
    "                                                               ['A', 'C', 'G', 'T'] if r[nt]),\n",
    "                                           axis=1) + ')'\n",
    "            )\n",
    "    .melt(id_vars=['sample', 'aligner', 'site', 'reference', 'consensus',\n",
    "                   'mutation', 'mutation_str'],\n",
    "          value_vars=comparators,\n",
    "          var_name='outgroup',\n",
    "          value_name='comparator_nt')\n",
    "    )\n",
    "\n",
    "deleted_diffs_all = (\n",
    "    deleted_diffs\n",
    "    [['sample', 'aligner', 'mutation', 'mutation_str']]\n",
    "    .drop_duplicates()\n",
    "    .groupby(['sample', 'aligner'], as_index=False)\n",
    "    .aggregate(substitutions=pd.NamedAgg('mutation', ','.join),\n",
    "               substitutions_str=pd.NamedAgg('mutation_str', ','.join),\n",
    "               )\n",
    "    .merge(deleted_alignment,\n",
    "           on=['sample', 'aligner'],\n",
    "           how='outer',\n",
    "           validate='many_to_one')\n",
    "    .assign(substitutions=lambda x: x['substitutions'].fillna(''),\n",
    "            substitutions_str=lambda x: x['substitutions_str'].fillna(''),\n",
    "            n_substitutions=lambda x: x['substitutions'].map(lambda subs: len([s for s in subs.split(',') if s]))\n",
    "            )\n",
    "    )\n",
    "\n",
    "deleted_diffs_to_outgroup = (\n",
    "    deleted_diffs\n",
    "    .query('comparator_nt == consensus')\n",
    "    .groupby(['sample', 'aligner', 'outgroup'], as_index=False)\n",
    "    .aggregate(substitutions_to_outgroup=pd.NamedAgg('mutation', ','.join),\n",
    "               substitutions_to_outgroup_str=pd.NamedAgg('mutation_str', ','.join)\n",
    "               )\n",
    "    .merge(pd.DataFrame(itertools.product(samples, aligners, comparators),\n",
    "                        columns=['sample', 'aligner', 'outgroup']),\n",
    "           how='outer',\n",
    "           on=['sample', 'aligner', 'outgroup'],\n",
    "           validate='one_to_many',\n",
    "           )\n",
    "    .assign(substitutions_to_outgroup=lambda x: x['substitutions_to_outgroup'].fillna(''),\n",
    "            substitutions_to_outgroup_str=lambda x: x['substitutions_to_outgroup_str'].fillna(''))\n",
    "    )\n",
    "\n",
    "deleted_diffs = deleted_diffs_all.merge(deleted_diffs_to_outgroup)\n",
    "\n",
    "deleted_deltas = pd.DataFrame()\n",
    "for comparator in comparators:\n",
    "    deleted_deltas = deleted_deltas.append(\n",
    "        deleted_diffs\n",
    "        [['sample', 'aligner', 'outgroup', 'substitutions']]\n",
    "        .query('outgroup == @comparator')\n",
    "        .assign(delta_distance_to_outgroup=lambda x: x['substitutions']\n",
    "                                                     .apply(delta_distance_comparator,\n",
    "                                                            args=(comparator,))\n",
    "                )\n",
    "        )\n",
    "    \n",
    "deleted_diffs = deleted_diffs.merge(deleted_deltas)\n",
    "\n",
    "deleted_diffs = deleted_diffs.assign(nt_28144=lambda x: x['sequence'].str[28143])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27cc9d-3d4c-46e0-842f-05f6154d93b6",
   "metadata": {},
   "source": [
    "Look at delta distance from reference.\n",
    "This is just a scratch chart for inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d57bf-07f7-4a82-98f7-83c1a08ae1aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aligner_selection = alt.selection_single(\n",
    "    name='read',\n",
    "    fields=['aligner'],\n",
    "    bind=alt.binding_select(options=aligners),\n",
    "    init={'aligner': aligners[0]},\n",
    "    )\n",
    "\n",
    "deleted_delta_chart = (\n",
    "    alt.Chart(deleted_diffs\n",
    "              .drop(columns=['sequence', 'sequence_region'])\n",
    "              .assign(delta_distance_to_outgroup=lambda x: x['delta_distance_to_outgroup'] + 0.05 * rand_jitter(len(x), seed=1))\n",
    "              )\n",
    "    .encode(x='frac_called_region',\n",
    "            y='delta_distance_to_outgroup',\n",
    "            color='patient_group',\n",
    "            tooltip=['sample',\n",
    "                     'n_substitutions',\n",
    "                     'substitutions_str',\n",
    "                     'substitutions_to_outgroup_str',\n",
    "                     'frac_called_region',\n",
    "                     'nt_28144',\n",
    "                     ]\n",
    "            )\n",
    "    .mark_point(filled=True,\n",
    "                size=50,\n",
    "                opacity=0.5)\n",
    "    .add_selection(outgroup_selection,\n",
    "                   aligner_selection)\n",
    "    .transform_filter(outgroup_selection)\n",
    "    .transform_filter(aligner_selection)\n",
    "    )\n",
    "\n",
    "deleted_delta_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbf540-dc1e-4e46-b613-15252075b653",
   "metadata": {},
   "source": [
    "For the rest of the analysis, we filter to just the samples with sufficient coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a43ba-b21b-43b4-a4d7-16174d287534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Just retaining samples with >={min_frac_coverage} coverage\")\n",
    "\n",
    "filtered_deleted_diffs = (\n",
    "    deleted_diffs\n",
    "    .query('frac_called_region >= @min_frac_coverage')\n",
    "    .assign(sample=lambda x: pd.Categorical(x['sample'], samples, ordered=True))\n",
    "    .sort_values('sample')\n",
    "    )\n",
    "\n",
    "filtered_deleted_diffs_display = (\n",
    "    filtered_deleted_diffs\n",
    "    [['sample', 'frac_called_region', 'patient_group', 'substitutions_str', 'n_substitutions']]\n",
    "    .drop_duplicates()\n",
    "    .assign(substitutions_str=lambda x: x['substitutions_str'].str.replace(',', ', '))\n",
    "    .rename(columns={'frac_called_region': f\"fraction sites called ({start}-{end})\", \n",
    "                     'patient_group': 'patient group',\n",
    "                     'n_substitutions': 'number of substitutions',\n",
    "                     'substitutions_str': f\"substitutions relative to {ref_genome_name}\"\n",
    "                     })\n",
    "    )\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "display(filtered_deleted_diffs_display)\n",
    "\n",
    "print(f\"Saving table to {deleted_diffs_latex}\")\n",
    "filtered_deleted_diffs_display.to_latex(deleted_diffs_latex,\n",
    "                                        float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c5b16c-cc19-4695-8c44-0d63b00b8f20",
   "metadata": {},
   "source": [
    "Also write the sequences for just the region of interest in this table to a FASTA file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f53a1-019f-4a6e-b119-92d80ebe4df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "assert 1 == filtered_deleted_diffs['aligner'].nunique(), f\"code below assumes one aligner: {filtered_deleted_diffs['aligner'].unique()}\"\n",
    "\n",
    "print(f\"Writing to {recovered_seqs_fasta}\")\n",
    "with open(recovered_seqs_fasta, 'w') as f:\n",
    "    for row in filtered_deleted_diffs[['sample', 'patient_group', 'sequence']].drop_duplicates().itertuples():\n",
    "        sample = row.sample\n",
    "        sequence = row.sequence[start - 1: end]\n",
    "        patient_group = {'early outpatient': 'early_Wuhan_epidemic',\n",
    "                         'hospital patient (Feb)': 'February_Renmin_Hospital'}[row.patient_group]\n",
    "        assert len(sequence) == end - start + 1\n",
    "        head = f\"{patient_group}/PRJNA612766_{sample}\"\n",
    "        f.write(f\">{head}\\n{sequence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56702d-fc27-48ea-9b73-853072c1358a",
   "metadata": {},
   "source": [
    "## Check if deleted sequences in GISAID downloaded ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f9316-b502-4346-80e2-e773f879b718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert 1 == filtered_deleted_diffs['aligner'].nunique()\n",
    "\n",
    "deleted_to_search_for = (\n",
    "    filtered_deleted_diffs\n",
    "    [['sample', 'substitutions']]\n",
    "    .drop_duplicates()\n",
    "    )\n",
    "\n",
    "strains_to_search_in = (\n",
    "    delta_dist_region\n",
    "    [['strain', 'substitutions']]\n",
    "    .drop_duplicates()\n",
    "    )\n",
    "\n",
    "records = []\n",
    "for tup in deleted_to_search_for.itertuples():\n",
    "    subs = tup.substitutions\n",
    "    matches = strains_to_search_in.query('substitutions == @subs')\n",
    "    records.append((tup.sample, subs, len(matches)))\n",
    "\n",
    "matches_in_gisaid = (\n",
    "    pd.DataFrame(records, columns=['sample', 'substitutions', 'n_matches'])\n",
    "    .sort_values('n_matches')\n",
    "    )\n",
    "\n",
    "display(matches_in_gisaid)\n",
    "\n",
    "matches_in_gisaid.to_csv(matches_in_gisaid_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11c275-0779-4d29-892e-178c8912f9ae",
   "metadata": {},
   "source": [
    "## Plot jitters of distance to outgroup\n",
    "First make data frame to plot that combines deleted data set and Wuhan early sequences (from January).\n",
    "Here we get the data for the deleted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceca393-7dec-4385-9648-4b9bad47bb40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(aligners) == 1, 'code below only works for one aligner, otherwise add aligner selection'\n",
    "\n",
    "deleted_jitter_df = (\n",
    "    filtered_deleted_diffs\n",
    "    .query('patient_group == \"early outpatient\"')\n",
    "    [['sample', 'patient_group', 'n_substitutions', 'substitutions',\n",
    "      'substitutions_to_outgroup', 'frac_called_region', 'outgroup',\n",
    "      'delta_distance_to_outgroup']]\n",
    "    .rename(columns={'sample': 'strain',\n",
    "                     'frac_called_region': 'frac_called',\n",
    "                     'patient_group': 'group'},\n",
    "            )\n",
    "    .assign(category='deleted early Wuhan',\n",
    "            huanan_market=False,\n",
    "            date='early Wuhan epidemic')\n",
    "    )\n",
    "\n",
    "deleted_jitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc53d2-af98-4c75-8c36-fbd692c2bcb8",
   "metadata": {},
   "source": [
    "For the early sequence set, just get sequences in January:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647dfc9b-3e4d-403c-8b99-87754a1ce66d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_seqs_jitter_df = (\n",
    "    delta_dist_region\n",
    "    .query('date <= @last_date')\n",
    "    .sort_values(['location_category', 'date'])\n",
    "    .assign(category=lambda x: x['location_subcategory'],\n",
    "            date=lambda x: x['date'].astype(str),\n",
    "            )\n",
    "    [['strain', 'n_substitutions', 'substitutions',\n",
    "      'substitutions_to_outgroup', 'frac_called', 'outgroup',\n",
    "      'delta_distance_to_outgroup', 'category',\n",
    "      'date']]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3db90b-c3ef-4c01-a875-5b7de6e5e5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jitter_df = pd.concat([deleted_jitter_df, early_seqs_jitter_df])\n",
    "\n",
    "y_extent = jitter_df['delta_distance_to_outgroup'].max() - jitter_df['delta_distance_to_outgroup'].min() + 1\n",
    "jitter_df = (\n",
    "    jitter_df\n",
    "    .assign(jitter_x=lambda x: 0.21 * rand_jitter(len(x), seed=4),\n",
    "            jitter_y=lambda x: y_extent / 20 * rand_jitter(len(x), seed=0)\n",
    "            )\n",
    "    )\n",
    "\n",
    "assert set(jitter_df['category']) == set(subcat_colors)\n",
    "\n",
    "dist_jitter_chart = (\n",
    "    alt.Chart(jitter_df)\n",
    "    .encode(column=alt.Column('category',\n",
    "                              title=None,\n",
    "                              header=alt.Header(labelAngle=-90,\n",
    "                                                labelOrient='bottom',\n",
    "                                                labelAlign='right',\n",
    "                                                labelPadding=3,\n",
    "                                                ),\n",
    "                              sort=list(subcat_colors),\n",
    "                              ),\n",
    "            x=alt.X('jitter_x:Q',\n",
    "                    title=None,\n",
    "                    axis=alt.Axis(values=[0], ticks=True, grid=False, labels=False),\n",
    "                    scale=alt.Scale(domain=[-1, 1]),\n",
    "                    ),\n",
    "            y=alt.Y('y:Q',\n",
    "                    title='relative mutations from outgroup',\n",
    "                    scale=alt.Scale(nice=False),\n",
    "                    axis=alt.Axis(tickMinStep=1),\n",
    "                    ),\n",
    "            color=alt.Color('category:N',\n",
    "                            scale=alt.Scale(range=list(subcat_colors.values())),\n",
    "                            legend=None,\n",
    "                            sort=list(subcat_colors),\n",
    "                            ),\n",
    "            tooltip=['strain',\n",
    "                     'date',\n",
    "                     alt.Tooltip('n_substitutions',\n",
    "                                 title='number substitutions'),\n",
    "                     'substitutions',\n",
    "                     alt.Tooltip('substitutions_to_outgroup',\n",
    "                                 title='substitutions to outgroup'),\n",
    "                     alt.Tooltip('frac_called',\n",
    "                                 title='fraction sites called',\n",
    "                                 format='.3f'),\n",
    "                     ],\n",
    "            )\n",
    "    .mark_point(filled=True,\n",
    "                opacity=0.45,\n",
    "                size=40,\n",
    "                )\n",
    "    .add_selection(outgroup_selection,\n",
    "                   jitter_slider)\n",
    "    .transform_filter(outgroup_selection)\n",
    "    .transform_calculate(\n",
    "            y='datum.delta_distance_to_outgroup + datum.jitter_y * y_axis_jitter.amount'\n",
    "            )\n",
    "    .configure_axis(grid=False)\n",
    "    .configure_view(stroke=None)\n",
    "    .configure_facet(spacing=0)\n",
    "    .properties(height=170,\n",
    "                width=40,\n",
    "                )\n",
    "    )\n",
    "\n",
    "for f in deltadist_jitter_charts:\n",
    "    print(f\"Saving to {f}\")\n",
    "    if os.path.splitext(f) == '.html':\n",
    "        dist_jitter_chart.save(f)\n",
    "    else:\n",
    "        altair_saver.save(dist_jitter_chart, f)\n",
    "\n",
    "dist_jitter_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4ca3f-279f-4e0b-bdbe-d5b234dbcefe",
   "metadata": {},
   "source": [
    "## Write out alignments\n",
    "For each alignment, we only get sequences that have unique substitutions relative to the reference, and collapse within these sets.\n",
    "\n",
    "First get data frames with the relevant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d804bab4-a0e0-414c-bf7f-320920d49bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs_all_d = {s.id: str(s.seq).upper()[ignore_muts_before - 1: ignore_muts_after]\n",
    "              for s in Bio.SeqIO.parse(early_seq_alignment_file, 'fasta')}\n",
    "\n",
    "all_alignment_df = (\n",
    "    delta_dist\n",
    "    .query('date <= @last_date')\n",
    "    .assign(strain_date=lambda x: x['strain'] + ' (' + x['date'].astype(str) + ')')\n",
    "    [['strain', 'strain_date', 'date', 'substitutions', 'frac_called', 'location_category', 'location_subcategory']]\n",
    "    .assign(sequence=lambda x: x['strain'].map(seqs_all_d))\n",
    "    .assign(site_offset=ignore_muts_before)\n",
    "    .query('frac_called >= @min_frac_called')\n",
    "    .drop_duplicates()\n",
    "    )\n",
    "\n",
    "seqs_region_d = {'early_Wuhan_epidemic/' + tup.sample: tup.sequence_region\n",
    "                 for tup in filtered_deleted_diffs.itertuples()}\n",
    "    \n",
    "deleted_alignment_df = (\n",
    "    deleted_jitter_df\n",
    "    [['strain', 'substitutions', 'frac_called']]\n",
    "    .assign(location_category='deleted early Wuhan',\n",
    "            location_subcategory='deleted early Wuhan',\n",
    "            strain=lambda x: 'early_Wuhan_epidemic/' + x['strain'].astype(str),\n",
    "            strain_date=lambda x: x['strain'],\n",
    "            date='early Wuhan epidemic')\n",
    "    .assign(sequence=lambda x: x['strain'].map(seqs_region_d))\n",
    "    .assign(site_offset=start)\n",
    "    .query('frac_called >= @min_frac_coverage')\n",
    "    .drop_duplicates()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463f799-a172-4f66-abc7-e2b2deedf569",
   "metadata": {},
   "source": [
    "Now remove any mutations in the list to ignore or that are rare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ca9e7-40e0-46a8-b71a-9bd60d5c629a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "muts_to_ignore = set(muts_to_ignore)\n",
    "print(f\"There are {len(muts_to_ignore)} mutations to ignore:\\n{muts_to_ignore}\")\n",
    "\n",
    "alignment_df = {'all': all_alignment_df.copy(),\n",
    "                'deleted': deleted_alignment_df.copy()}\n",
    "\n",
    "def filter_seq(seq, subs_to_remove, site_offset):\n",
    "    seq = list(seq)\n",
    "    for s in subs_to_remove:\n",
    "        wt, site, mut = s[0], int(s[1: -1]) - site_offset, s[-1]\n",
    "        assert seq[site] == mut, f\"{seq[site]=}, {wt=}, {s=}\"\n",
    "        seq[site] = wt\n",
    "    return ''.join(seq)\n",
    "\n",
    "singletons = {s for s, n in \n",
    "              collections.Counter([s for s in\n",
    "                                   ','.join(alignment_df['all']['substitutions'].tolist() +\n",
    "                                            alignment_df['deleted']['substitutions'].tolist()\n",
    "                                            ).split(',')\n",
    "                                   if s]\n",
    "                                  ).items()\n",
    "              if n <= collapse_rare_muts\n",
    "              }\n",
    "print(f\"There are {len(singletons)} mutations to collapse \"\n",
    "      f\"because they are found <= {collapse_rare_muts} times\")\n",
    "\n",
    "for desc in ['all', 'deleted']:\n",
    "    alignment_df[desc] = (\n",
    "        alignment_df[desc]\n",
    "        .assign(substitutions_to_remove=lambda x: x['substitutions'].map(\n",
    "                        lambda subs: {s for s in subs.split(',')\n",
    "                                      if s in muts_to_ignore or s in singletons}\n",
    "                        ),\n",
    "                n_substitutions_removed=lambda x: x['substitutions_to_remove'].map(len),\n",
    "                substitutions=lambda x: x.apply(lambda r: ','.join([s for s in r['substitutions'].split(',')\n",
    "                                                                    if s not in r['substitutions_to_remove']]),\n",
    "                                                axis=1),\n",
    "                sequence=lambda x: x.apply(lambda r: filter_seq(r['sequence'],\n",
    "                                                                r['substitutions_to_remove'],\n",
    "                                                                r['site_offset']),\n",
    "                                           axis=1),\n",
    "                )\n",
    "        .drop(columns='substitutions_to_remove')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6421df-8d9f-4943-85fb-02b65f3425be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T01:25:18.888267Z",
     "iopub.status.busy": "2021-06-09T01:25:18.887777Z",
     "iopub.status.idle": "2021-06-09T01:25:18.900617Z",
     "shell.execute_reply": "2021-06-09T01:25:18.899739Z",
     "shell.execute_reply.started": "2021-06-09T01:25:18.888221Z"
    },
    "tags": []
   },
   "source": [
    "Now just get one representative sequence from each set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75855f-8e99-4faa-bd1b-996c62874cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for desc in ['all', 'deleted']:\n",
    "    print(f\"For {desc}, starting with {len(alignment_df[desc])} sequences\")\n",
    "    assert all(alignment_df[desc]['sequence'].map(len) ==\n",
    "               alignment_df[desc]['sequence'].map(len).values[0])\n",
    "    locations = alignment_df[desc]['location_category'].unique().tolist()\n",
    "    location_subcategories = alignment_df[desc]['location_subcategory'].unique().tolist()\n",
    "    for location in locations:\n",
    "        alignment_df[desc][f\"cat_{location}\"] = (alignment_df[desc]['location_category']\n",
    "                                                 == location).astype(int)\n",
    "    for sublocation in location_subcategories:\n",
    "        alignment_df[desc][f\"subcat_{sublocation}\"] = (alignment_df[desc]['location_subcategory']\n",
    "                                                       == sublocation).astype(int)\n",
    "    alignment_df[desc] = (\n",
    "        alignment_df[desc]\n",
    "        .sort_values(['date', 'n_substitutions_removed', 'frac_called'],\n",
    "                     ascending=[True, True, False])\n",
    "        .groupby('substitutions', as_index=False)\n",
    "        .aggregate(nstrains=pd.NamedAgg('strain', 'count'),\n",
    "                   representative_strain=pd.NamedAgg('strain', 'first'),\n",
    "                   sequence=pd.NamedAgg('sequence', 'first'),\n",
    "                   all_strains=pd.NamedAgg('strain', ', '.join),\n",
    "                   all_strains_dates=pd.NamedAgg('strain_date', ', '.join),\n",
    "                   **{f\"cat_{location}\": pd.NamedAgg(f\"cat_{location}\", 'sum')\n",
    "                      for location in locations},\n",
    "                   **{f\"subcat_{location_subcat}\": pd.NamedAgg(f\"subcat_{location_subcat}\", 'sum')\n",
    "                      for location_subcat in location_subcategories},\n",
    "                   )\n",
    "        )\n",
    "    print(f\"After collapsing, have {len(alignment_df[desc])} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aade0cf-527a-41d3-b947-fcbca7ac3a53",
   "metadata": {},
   "source": [
    "Now add delta distances to outgroup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959b03e-c559-4a2f-9710-a7d79918824c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for desc in ['all']:\n",
    "    for outgroup in comparators:\n",
    "        alignment_df[desc][f\"{outgroup}_delta_dist\"] = (\n",
    "            alignment_df[desc]\n",
    "            ['substitutions']\n",
    "            .apply(delta_distance_comparator, args=(comparator,))\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8394c81-9533-46bc-8e80-c6a093fabb7b",
   "metadata": {},
   "source": [
    "Write the alignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3406a38-cb76-41c3-a436-e41f96850700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for desc, alignment_file, alignment_csv, alignment_file_no_filter, alignment_csv_no_filter in [\n",
    "        ('all', alignment_all_fasta, alignment_all_csv, alignment_all_no_filter_rare_fasta, alignment_all_no_filter_rare_csv),\n",
    "        ('deleted', None, None, None, deleted_csv)\n",
    "        ]:\n",
    "    df_no_filter = alignment_df[desc]\n",
    "    df_filter = df_no_filter.query('nstrains > @filter_rare_variants')\n",
    "    print(f\"\\nThere are {len(df_no_filter)} total sequences in the {desc} category\")\n",
    "    print(f\"There are {len(df_filter)} of the sequences remaining after filtering out ones observed <= {filter_rare_variants} times\")\n",
    "    for df, fasta_file, csv_file in [(df_no_filter, alignment_file_no_filter, alignment_csv_no_filter),\n",
    "                                     (df_filter, alignment_file, alignment_csv)]:\n",
    "        if csv_file:\n",
    "            print(f\"Writing to {csv_file}\")\n",
    "            df.drop(columns='sequence').to_csv(csv_file, index=False)\n",
    "        if fasta_file:\n",
    "            print(f\"Writing to {fasta_file}\")\n",
    "            a = [Bio.SeqRecord.SeqRecord(seq=Bio.Seq.Seq(tup.sequence),\n",
    "                                         id=tup.representative_strain,\n",
    "                                         name='',\n",
    "                                         description='')\n",
    "                 for tup in df.itertuples()]\n",
    "            Bio.SeqIO.write(a, fasta_file, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4907c7-c1a8-4aae-9b6d-30e2c8d512fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
