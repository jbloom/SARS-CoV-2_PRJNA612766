{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6058efba-4ca7-46a3-ae3b-7d1293e15fea",
   "metadata": {},
   "source": [
    "# Analyze distance to outgroup comparators\n",
    "\n",
    "Get variables from `snakemake`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a936869-3d81-455f-aed0-f53844324b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_seq_subs_file = snakemake.input.early_seq_subs\n",
    "early_seq_alignment_file = snakemake.input.early_seq_alignment\n",
    "deleted_diffs_file = snakemake.input.deleted_diffs\n",
    "deleted_alignment_file = snakemake.input.deleted_consensus\n",
    "comparator_map_file = snakemake.input.comparator_map\n",
    "region_of_interest = snakemake.params.region_of_interest\n",
    "comparators = snakemake.params.comparators\n",
    "min_frac_coverage = snakemake.params.min_frac_coverage\n",
    "samples = snakemake.params.samples\n",
    "aligners = snakemake.params.aligners\n",
    "ref_genome_name = snakemake.params.ref_genome_name\n",
    "ignore_muts_before = snakemake.params.ignore_muts_before\n",
    "ignore_muts_after = snakemake.params.ignore_muts_after\n",
    "last_date = snakemake.params.phylo_last_date\n",
    "muts_to_ignore = snakemake.params.phylo_muts_to_ignore\n",
    "collapse_rare_muts = snakemake.params.phylo_collapse_rare_muts\n",
    "filter_rare_variants = snakemake.params.phylo_filter_rare_variants\n",
    "min_frac_called = snakemake.params.phylo_min_frac_called\n",
    "\n",
    "alignment_all_fasta = snakemake.output.alignment_all_fasta\n",
    "alignment_region_fasta = snakemake.output.alignment_region_fasta\n",
    "alignment_all_csv = snakemake.output.alignment_all_csv\n",
    "alignment_region_csv = snakemake.output.alignment_region_csv\n",
    "early_seq_count_charts = snakemake.output.early_seq_counts\n",
    "early_seq_deltadist_charts = snakemake.output.early_seq_deltadist\n",
    "early_seq_deltadist_region_charts = snakemake.output.early_seq_deltadist_region\n",
    "deltadist_jitter_charts = snakemake.output.deltadist_jitter\n",
    "deleted_diffs_latex = snakemake.output.deleted_diffs_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709fa86c-ceca-4ed2-b9da-3bd2abd64437",
   "metadata": {},
   "source": [
    "Import Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881c8b6-495b-499f-aaa0-cfd1acd1b4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "import altair_saver\n",
    "\n",
    "import Bio.SeqIO\n",
    "\n",
    "import numpy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "_ = alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af792c-7f6c-4586-81f0-1bb15fe4ff57",
   "metadata": {},
   "source": [
    "## Early GISAID sequences\n",
    "Read early sequence substitutions and comparator map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d31d5-68df-47c7-8900-e449220dba89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_seq_subs = (\n",
    "    pd.read_csv(early_seq_subs_file, na_filter=None)\n",
    "    .assign(date=lambda x: pd.to_datetime(x['date']))\n",
    "    )\n",
    "\n",
    "comparator_map = pd.read_csv(comparator_map_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06008f90-e1cb-4067-90c8-9032774d7cc0",
   "metadata": {},
   "source": [
    "Annotate whether strains from Wuhan, elsewhere in China, or outside China:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c14873-36cd-4712-986c-2dfe60e84da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert early_seq_subs['country'].notnull().all()\n",
    "\n",
    "def location_category(row):\n",
    "    if 'Wuhan' == row['location']:\n",
    "        return 'Wuhan'\n",
    "    elif 'Wuhan' in row['strain'] and 'Beijing' not in row['strain']:\n",
    "        # there are some strains with hCoV-19/Beijing/Wuhan_IME-BJ01/2020\n",
    "        # which appear to actually be from Beijing according to most annotations\n",
    "        return 'Wuhan'\n",
    "    elif 'China' in row['country']:\n",
    "        return 'other China'\n",
    "    else:\n",
    "        return 'outside China'\n",
    "\n",
    "early_seq_subs['location_category'] = early_seq_subs.apply(location_category, axis=1)\n",
    "\n",
    "early_seq_subs.groupby('location_category').aggregate(n_seqs=pd.NamedAgg('strain', 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8cbe21-d335-4b16-b326-d148a75c3d11",
   "metadata": {},
   "source": [
    "Plot number of sequences from each location as function of date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a41be-9650-4bee-b4cc-6201a57f6463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get counts per week as here, with counts being the week\n",
    "# before that date\n",
    "location_date_df = (\n",
    "    early_seq_subs\n",
    "    [['strain', 'location_category', 'date']]\n",
    "    # https://stackoverflow.com/a/45281439/4191652\n",
    "    .groupby(['location_category',\n",
    "              pd.Grouper(key='date', freq='W-FRI')],\n",
    "              )\n",
    "    .aggregate(nseqs=pd.NamedAgg('strain', 'count'))\n",
    "    .reset_index()\n",
    "    )\n",
    "\n",
    "# make chart\n",
    "location_date_chart = (\n",
    "    alt.Chart(location_date_df)\n",
    "    .encode(x=alt.X('date:T',\n",
    "                    axis=alt.Axis(labelAngle=-90,\n",
    "                                  values=location_date_df['date'].unique(),\n",
    "                                  format='%b %d',\n",
    "                                  tickCount=location_date_df['date'].nunique(),\n",
    "                                  ),\n",
    "                    title='week in 2019 or 2020',\n",
    "                    ),\n",
    "            y=alt.Y('nseqs',\n",
    "                    title='sequences from prior week',\n",
    "                    ),\n",
    "            color=alt.Color('location_category:N',\n",
    "                            legend=alt.Legend(title='location'),\n",
    "                            scale=alt.Scale(range=['#E69F00', '#009E73', '#F0E442'])\n",
    "                            ),\n",
    "            tooltip=['date',\n",
    "                     alt.Tooltip('nseqs',\n",
    "                                 title='number of sequences',\n",
    "                                 ),\n",
    "                     alt.Tooltip('location_category',\n",
    "                                 title='location',\n",
    "                                 ),\n",
    "                     ],\n",
    "            )\n",
    "    .mark_line(point=True)\n",
    "    .configure_point(size=65)\n",
    "    .configure_axis(grid=False)\n",
    "    .properties(width=275,\n",
    "                height=150,\n",
    "                )\n",
    "    )\n",
    "\n",
    "for f in early_seq_count_charts:\n",
    "    print(f\"Saving to {f}\")\n",
    "    if os.path.splitext(f) == '.html':\n",
    "        location_date_chart.save(f)\n",
    "    else:\n",
    "        altair_saver.save(location_date_chart, f)\n",
    "\n",
    "location_date_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d57f3f-d245-485d-bb70-5804df0de01f",
   "metadata": {},
   "source": [
    "Define a function that computes the total change in Hamming distance from each comparator relative to the reference based on the substitutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee995e1b-5705-4209-a853-2699d2d48d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_to_ref = comparator_map.set_index('site')['reference'].to_dict()\n",
    "comparator_site_to_nt = {comparator: comparator_map.set_index('site')[comparator].to_dict()\n",
    "                         for comparator in comparators}\n",
    "\n",
    "def delta_distance_comparator(subs_str, comparator):\n",
    "    \"\"\"Total change in Hamming distance from comparator relative to reference.\"\"\"\n",
    "    site_to_nt = comparator_site_to_nt[comparator]\n",
    "    n = 0\n",
    "    for s in [s for s in subs_str.split(',') if s]:\n",
    "        m = re.fullmatch('(?P<wt>[ACGT])(?P<site>\\d+)(?P<mut>[ACGT])', s)\n",
    "        if not m:\n",
    "            raise ValueError(f\"cannot match {s}\")\n",
    "        wt = m.group('wt')\n",
    "        site = int(m.group('site'))\n",
    "        mut = m.group('mut')\n",
    "        assert site_to_ref[site] == wt\n",
    "        comp = site_to_nt[site]\n",
    "        if comp in 'ACGT':\n",
    "            if mut == comp:\n",
    "                n -= 1\n",
    "            elif mut != comp:\n",
    "                n += 1\n",
    "        elif comp not in ['-', 'N']:\n",
    "            raise ValueError(f\"invalid comparator identity {comp}\")\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3451e-973d-47af-8ed2-8ba220423cc8",
   "metadata": {},
   "source": [
    "Apply this function to each comparator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3daea0-e164-4f88-9fdf-2d461044cc98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for comparator in comparators:\n",
    "    early_seq_subs[f\"{comparator}_delta_dist\"] = (early_seq_subs\n",
    "                                                  ['substitutions']\n",
    "                                                  .apply(delta_distance_comparator,\n",
    "                                                         args=(comparator,)\n",
    "                                                         )\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b26c15-a97b-4a76-b03f-d1491f15f048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T14:50:15.529172Z",
     "iopub.status.busy": "2021-06-05T14:50:15.528973Z",
     "iopub.status.idle": "2021-06-05T14:50:15.568412Z",
     "shell.execute_reply": "2021-06-05T14:50:15.567808Z",
     "shell.execute_reply.started": "2021-06-05T14:50:15.529152Z"
    },
    "tags": []
   },
   "source": [
    "Make a tidy data frame with these delta distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e66bc-2be5-4a03-9402-7667ff82997d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(early_seq_subs) == early_seq_subs['strain'].nunique()\n",
    "\n",
    "delta_dist = (\n",
    "    early_seq_subs\n",
    "    .melt(id_vars=['strain', 'gisaid_epi_isl', 'date', 'location_category',\n",
    "                   'frac_called', 'substitutions', 'huanan_market'],\n",
    "          value_vars=[f\"{comparator}_substitutions\" for comparator in comparators],\n",
    "          var_name='outgroup',\n",
    "          value_name='substitutions_to_outgroup')\n",
    "    .assign(outgroup=lambda x: x['outgroup'].str.replace('_substitutions', ''),\n",
    "            n_substitutions=lambda x: x['substitutions'].map(lambda s: len([s for s in s.split(',') if s])),\n",
    "            )\n",
    "    .merge(early_seq_subs.melt(id_vars='strain',\n",
    "                               value_vars=[f\"{comparator}_delta_dist\" for\n",
    "                                           comparator in comparators],\n",
    "                               var_name='outgroup',\n",
    "                               value_name='delta_distance_to_outgroup',\n",
    "                               )\n",
    "                         .assign(outgroup=lambda x: x['outgroup'].str\n",
    "                                                    .replace('_delta_dist', '')\n",
    "                                 ),\n",
    "           on=['strain', 'outgroup'],\n",
    "           validate='one_to_one',\n",
    "           )\n",
    "    )\n",
    "\n",
    "assert len(delta_dist) == len(early_seq_subs) * len(comparators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e863f9-89fb-4376-81f4-f52b2fa5d9e3",
   "metadata": {},
   "source": [
    "Now do the same thing just for the region of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50687ab2-ee3e-4f6c-94b2-7b7dfdc20bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = region_of_interest['start']\n",
    "end = region_of_interest['end']\n",
    "def subs_in_region(subs_str):\n",
    "    return ','.join(s for s in subs_str.split(',')\n",
    "                    if s and start <= int(s[1 : -1]) <= end)\n",
    "    \n",
    "early_seq_subs_region = (\n",
    "    early_seq_subs\n",
    "    .assign(substitutions=lambda x: x['substitutions'].map(subs_in_region),\n",
    "            frac_called=lambda x: x['frac_called_in_region_of_interest'])\n",
    "    )\n",
    "\n",
    "for comparator in comparators:\n",
    "    early_seq_subs_region[f\"{comparator}_delta_dist\"] = (\n",
    "                                                early_seq_subs_region\n",
    "                                                ['substitutions']\n",
    "                                                .apply(delta_distance_comparator,\n",
    "                                                       args=(comparator,)\n",
    "                                                       )\n",
    "                                                )\n",
    "    early_seq_subs_region[f\"{comparator}_substitutions\"] = (\n",
    "                        early_seq_subs_region\n",
    "                        [f\"{comparator}_substitutions\"]\n",
    "                        .map(subs_in_region)\n",
    "                        )\n",
    "    \n",
    "delta_dist_region = (\n",
    "    early_seq_subs_region\n",
    "    .melt(id_vars=['strain', 'gisaid_epi_isl', 'date', 'location_category',\n",
    "                   'frac_called', 'substitutions', 'huanan_market'],\n",
    "          value_vars=[f\"{comparator}_substitutions\" for comparator in comparators],\n",
    "          var_name='outgroup',\n",
    "          value_name='substitutions_to_outgroup')\n",
    "    .assign(outgroup=lambda x: x['outgroup'].str.replace('_substitutions', ''),\n",
    "            n_substitutions=lambda x: x['substitutions'].map(lambda s: len([s for s in s.split(',') if s])),\n",
    "            )\n",
    "    .merge(early_seq_subs_region.melt(id_vars='strain',\n",
    "                                      value_vars=[f\"{comparator}_delta_dist\" for\n",
    "                                                  comparator in comparators],\n",
    "                                      var_name='outgroup',\n",
    "                                      value_name='delta_distance_to_outgroup',\n",
    "                                      )\n",
    "                                .assign(outgroup=lambda x: x['outgroup'].str\n",
    "                                                           .replace('_delta_dist', '')\n",
    "                                        ),\n",
    "           on=['strain', 'outgroup'],\n",
    "           validate='one_to_one',\n",
    "           )\n",
    "    )\n",
    "\n",
    "assert len(delta_dist_region) == len(early_seq_subs) * len(comparators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da864f6-3e31-427f-981f-7225cefa7caf",
   "metadata": {},
   "source": [
    "Get identity at site 28144 and add to data frame of delta distances for region of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8dd8d-6f97-42b8-9386-4cd2af56b81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nt_28144 = {}\n",
    "for s in Bio.SeqIO.parse(early_seq_alignment_file, 'fasta'):\n",
    "    nt_28144[s.id] = str(s.seq[28143]).upper()\n",
    "\n",
    "delta_dist_region = (\n",
    "    delta_dist_region\n",
    "    .assign(nt_28144=lambda x: x['strain'].map(nt_28144))\n",
    "    )\n",
    "assert delta_dist_region['nt_28144'].notnull().all()\n",
    "\n",
    "display(delta_dist_region\n",
    "        [['strain', 'location_category', 'nt_28144']]\n",
    "        .drop_duplicates()\n",
    "        .groupby(['location_category', 'nt_28144'])\n",
    "        .aggregate(n_strains=pd.NamedAgg('strain', 'count'))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30694d9a-4229-4b66-9941-a33dffe6314e",
   "metadata": {},
   "source": [
    "Function to plot delta distances versus date of isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a82a07-87a7-4f37-89ad-fc5c6cc4c98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_dist_chart_height = 170\n",
    "\n",
    "outgroup_selection = alt.selection_single(\n",
    "    name='sequence',\n",
    "    fields=['outgroup'],\n",
    "    bind=alt.binding_select(options=comparators),\n",
    "    init={'outgroup': comparators[0]},\n",
    "    )\n",
    "\n",
    "jitter_slider = alt.selection_single(\n",
    "        name='y_axis_jitter',\n",
    "        fields=['amount'],\n",
    "        init={'amount': 0.2},\n",
    "        bind=alt.binding_range(min=0, max=1)\n",
    "        )\n",
    "\n",
    "def rand_jitter(n, seed):\n",
    "    \"\"\"Calculate random jitter as here:\n",
    "    https://www.geeksforgeeks.org/how-to-make-stripplot-with-jitter-in-altair-python/\n",
    "    \"\"\"\n",
    "    numpy.random.seed(seed)\n",
    "    return (numpy.sqrt(-2 * numpy.log(numpy.random.rand(n))) *\n",
    "            numpy.cos(2 * numpy.pi * numpy.random.rand(n)))\n",
    "\n",
    "def get_delta_distance_plot(df):\n",
    "    \n",
    "    y_extent = df['delta_distance_to_outgroup'].max() - df['delta_distance_to_outgroup'].min()\n",
    "    \n",
    "    delta_distance_points = (\n",
    "        alt.Chart(df\n",
    "                  .assign(jitter_y=y_extent / 20 * rand_jitter(len(df), seed=1))\n",
    "                  )\n",
    "        .encode(x=alt.X('date:T',\n",
    "                        axis=alt.Axis(labelAngle=-90),\n",
    "                        title='date in 2019 or 2020',\n",
    "                        ),\n",
    "                y=alt.Y('y:Q',\n",
    "                        title='relative mutations from outgroup',\n",
    "                        scale=alt.Scale(nice=False),\n",
    "                        axis=alt.Axis(tickMinStep=1),\n",
    "                        ),\n",
    "                color=alt.Color('location_category:N',\n",
    "                                scale=alt.Scale(range=['#E69F00', '#009E73', '#F0E442']),\n",
    "                                legend=None,\n",
    "                                ),\n",
    "                shape=alt.Shape('huanan_market:N',\n",
    "                                legend=alt.Legend(orient='top',\n",
    "                                                  symbolFillColor='#E69F00',\n",
    "                                                  symbolStrokeColor='#E69F00',\n",
    "                                                  offset=0,\n",
    "                                                  symbolSize=50,\n",
    "                                                  ),\n",
    "                                scale=alt.Scale(range=['circle', 'square']),\n",
    "                                title='from seafood market',\n",
    "                                ),\n",
    "                tooltip=['strain',\n",
    "                         alt.Tooltip('gisaid_epi_isl',\n",
    "                                     title='GISAID ID'),\n",
    "                         'date',\n",
    "                         alt.Tooltip('n_substitutions',\n",
    "                                     title='number substitutions'),\n",
    "                         'substitutions',\n",
    "                         alt.Tooltip('substitutions_to_outgroup',\n",
    "                                     title='substitutions to outgroup'),\n",
    "                         alt.Tooltip('frac_called',\n",
    "                                     title='fraction sites called',\n",
    "                                     format='.3f'),\n",
    "                         alt.Tooltip('huanan_market:N',\n",
    "                                     title='from seafood market'),\n",
    "                         ],\n",
    "                )\n",
    "        .mark_point(filled=True,\n",
    "                    opacity=0.5,\n",
    "                    size=30,\n",
    "                    )\n",
    "        .transform_filter(outgroup_selection)\n",
    "        .transform_calculate(\n",
    "            y='datum.delta_distance_to_outgroup + datum.jitter_y * y_axis_jitter.amount'\n",
    "            )\n",
    "        .properties(height=delta_dist_chart_height,\n",
    "                    width=270,\n",
    "                    )\n",
    "        )\n",
    "\n",
    "    delta_distance_lines = (\n",
    "        delta_distance_points\n",
    "        .transform_regression('date', 'delta_distance_to_outgroup',\n",
    "                              groupby=['location_category'])\n",
    "        .encode(color=alt.value('#999999'),\n",
    "                y='delta_distance_to_outgroup')\n",
    "        .mark_line(opacity=0.3,\n",
    "                   size=5,\n",
    "                   point=False,\n",
    "                   )\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        (delta_distance_points + delta_distance_lines)\n",
    "        .add_selection(jitter_slider,\n",
    "                       outgroup_selection,\n",
    "                       )\n",
    "        .facet(facet=alt.Facet('location_category:N',\n",
    "                               title=None,\n",
    "                               header=alt.Header(labelFontStyle='bold',\n",
    "                                                 labelPadding=1,\n",
    "                                                 labelFontSize=12,\n",
    "                                                 ),\n",
    "                               ),\n",
    "               columns=3,\n",
    "               spacing=2,\n",
    "               )\n",
    "        .configure_axis(grid=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54799d0f-867c-46e6-b8b4-bb6f9cba1f9c",
   "metadata": {},
   "source": [
    "Make chart for whole genome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8234e-99a4-4298-8309-16254df604ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_distance_all_chart = get_delta_distance_plot(delta_dist)\n",
    "\n",
    "for f in early_seq_deltadist_charts:\n",
    "    print(f\"Saving to {f}\")\n",
    "    if os.path.splitext(f) == '.html':\n",
    "        delta_distance_all_chart.save(f)\n",
    "    else:\n",
    "        altair_saver.save(delta_distance_all_chart, f)\n",
    "\n",
    "delta_distance_all_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3b437-a32e-4b92-b5e2-1eafe8262537",
   "metadata": {},
   "source": [
    "Make chart for region of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca0af5-5618-42b5-9f9f-aa2a3ec5f31e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_distance_region_chart = get_delta_distance_plot(delta_dist_region)\n",
    "\n",
    "for f in early_seq_deltadist_region_charts:\n",
    "    print(f\"Saving to {f}\")\n",
    "    if os.path.splitext(f) == '.html':\n",
    "        delta_distance_region_chart.save(f)\n",
    "    else:\n",
    "        altair_saver.save(delta_distance_region_chart, f)\n",
    "\n",
    "delta_distance_region_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4004f0-6a83-4fda-8fd8-e8e69051d4a2",
   "metadata": {},
   "source": [
    "## Read the alignment of all early seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72df7c1-b7a6-47cc-832b-9763fe917c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_seq_alignment = list(Bio.SeqIO.parse(early_seq_alignment_file, 'fasta'))\n",
    "\n",
    "aligned_length = len(early_seq_alignment[0])\n",
    "print(f\"Alignment length is {aligned_length}\")\n",
    "\n",
    "assert all(len(s) == aligned_length for s in early_seq_alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f144d5-b406-4dc6-8280-f331487eba1d",
   "metadata": {},
   "source": [
    "## Deleted sequence set\n",
    "Get information on substitutions in deleted sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a18a01-6b72-453e-860e-c89faa128a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deleted_diffs = pd.read_csv(deleted_diffs_file)\n",
    "\n",
    "deleted_alignment = pd.read_csv(deleted_alignment_file)\n",
    "\n",
    "# make sure we have information for the expected samples / aligners\n",
    "expect_samples_aligners = {(s, a) for s, a in itertools.product(samples, aligners)}\n",
    "assert expect_samples_aligners == set(deleted_alignment[['sample', 'aligner']]\n",
    "                                      .itertuples(index=False, name=None))\n",
    "assert expect_samples_aligners.issuperset(deleted_diffs[['sample', 'aligner']]\n",
    "                                          .itertuples(index=False, name=None))\n",
    "\n",
    "# make sure alignment of correct length\n",
    "assert all(deleted_alignment['sequence'].map(len) == aligned_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d971b3-96f4-4faa-a955-a5a8356b3bfd",
   "metadata": {},
   "source": [
    "Get the region of interest sequence and fraction sites called in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9a6bb-6f11-4e30-b4b3-607a92321734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_groups = {sample: d['patient_group'] for sample, d in samples.items()}\n",
    "\n",
    "deleted_alignment = (\n",
    "    deleted_alignment\n",
    "    .assign(sequence_region=lambda x: x['sequence'].str[start - 1: end],\n",
    "            frac_called_region=lambda x: x['sequence_region']\n",
    "                                         .map(lambda s: sum(nt in 'ACGT' for nt in s) /\n",
    "                                                        (end - start + 1)\n",
    "                                              ),\n",
    "            patient_group=lambda x: x['sample'].map(patient_groups)\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b03eb-786a-46a9-9d97-3c8ee68e20ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:49.400144Z",
     "iopub.status.busy": "2021-06-07T00:10:49.399488Z",
     "iopub.status.idle": "2021-06-07T00:10:49.416614Z",
     "shell.execute_reply": "2021-06-07T00:10:49.415638Z",
     "shell.execute_reply.started": "2021-06-07T00:10:49.400071Z"
    }
   },
   "source": [
    "Get the differences from the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a6ab3-46af-4df3-9c29-a1ac0fecad00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Reading diffs from ref from {deleted_diffs_file}, subsetting to sites {start} to {end}\")\n",
    "\n",
    "deleted_diffs = (\n",
    "    pd.read_csv(deleted_diffs_file)\n",
    "    .query('(site >= @start) and (site <= @end)')\n",
    "    .assign(mutation=lambda x: x['reference'] + x['site'].astype(str) + x['consensus'],\n",
    "            mutation_str=lambda x: x['mutation'] + '(' +\n",
    "                                   x.apply(lambda r: ','.join(f\"{nt}={r[nt]}\" for nt in\n",
    "                                                               ['A', 'C', 'G', 'T'] if r[nt]),\n",
    "                                           axis=1) + ')'\n",
    "            )\n",
    "    .melt(id_vars=['sample', 'aligner', 'site', 'reference', 'consensus',\n",
    "                   'mutation', 'mutation_str'],\n",
    "          value_vars=comparators,\n",
    "          var_name='outgroup',\n",
    "          value_name='comparator_nt')\n",
    "    )\n",
    "\n",
    "deleted_diffs_all = (\n",
    "    deleted_diffs\n",
    "    [['sample', 'aligner', 'mutation', 'mutation_str']]\n",
    "    .drop_duplicates()\n",
    "    .groupby(['sample', 'aligner'], as_index=False)\n",
    "    .aggregate(substitutions=pd.NamedAgg('mutation', ','.join),\n",
    "               substitutions_str=pd.NamedAgg('mutation_str', ','.join),\n",
    "               )\n",
    "    .merge(deleted_alignment,\n",
    "           on=['sample', 'aligner'],\n",
    "           how='outer',\n",
    "           validate='many_to_one')\n",
    "    .assign(substitutions=lambda x: x['substitutions'].fillna(''),\n",
    "            substitutions_str=lambda x: x['substitutions_str'].fillna(''),\n",
    "            n_substitutions=lambda x: x['substitutions'].map(lambda subs: len([s for s in subs.split(',') if s]))\n",
    "            )\n",
    "    )\n",
    "\n",
    "deleted_diffs_to_outgroup = (\n",
    "    deleted_diffs\n",
    "    .query('comparator_nt == consensus')\n",
    "    .groupby(['sample', 'aligner', 'outgroup'], as_index=False)\n",
    "    .aggregate(substitutions_to_outgroup=pd.NamedAgg('mutation', ','.join),\n",
    "               substitutions_to_outgroup_str=pd.NamedAgg('mutation_str', ','.join)\n",
    "               )\n",
    "    .merge(pd.DataFrame(itertools.product(samples, aligners, comparators),\n",
    "                        columns=['sample', 'aligner', 'outgroup']),\n",
    "           how='outer',\n",
    "           on=['sample', 'aligner', 'outgroup'],\n",
    "           validate='one_to_many',\n",
    "           )\n",
    "    .assign(substitutions_to_outgroup=lambda x: x['substitutions_to_outgroup'].fillna(''),\n",
    "            substitutions_to_outgroup_str=lambda x: x['substitutions_to_outgroup_str'].fillna(''))\n",
    "    )\n",
    "\n",
    "deleted_diffs = deleted_diffs_all.merge(deleted_diffs_to_outgroup)\n",
    "\n",
    "deleted_deltas = pd.DataFrame()\n",
    "for comparator in comparators:\n",
    "    deleted_deltas = deleted_deltas.append(\n",
    "        deleted_diffs\n",
    "        [['sample', 'aligner', 'outgroup', 'substitutions']]\n",
    "        .query('outgroup == @comparator')\n",
    "        .assign(delta_distance_to_outgroup=lambda x: x['substitutions']\n",
    "                                                     .apply(delta_distance_comparator,\n",
    "                                                            args=(comparator,))\n",
    "                )\n",
    "        )\n",
    "    \n",
    "deleted_diffs = deleted_diffs.merge(deleted_deltas)\n",
    "\n",
    "deleted_diffs = deleted_diffs.assign(nt_28144=lambda x: x['sequence'].str[28143])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27cc9d-3d4c-46e0-842f-05f6154d93b6",
   "metadata": {},
   "source": [
    "Look at delta distance from reference.\n",
    "This is just a scratch chart for inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d57bf-07f7-4a82-98f7-83c1a08ae1aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aligner_selection = alt.selection_single(\n",
    "    name='read',\n",
    "    fields=['aligner'],\n",
    "    bind=alt.binding_select(options=aligners),\n",
    "    init={'aligner': aligners[0]},\n",
    "    )\n",
    "\n",
    "deleted_delta_chart = (\n",
    "    alt.Chart(deleted_diffs\n",
    "              .drop(columns=['sequence', 'sequence_region'])\n",
    "              .assign(delta_distance_to_outgroup=lambda x: x['delta_distance_to_outgroup'] + 0.05 * rand_jitter(len(x), seed=1))\n",
    "              )\n",
    "    .encode(x='frac_called_region',\n",
    "            y='delta_distance_to_outgroup',\n",
    "            color='patient_group',\n",
    "            tooltip=['sample',\n",
    "                     'n_substitutions',\n",
    "                     'substitutions_str',\n",
    "                     'substitutions_to_outgroup_str',\n",
    "                     'frac_called_region',\n",
    "                     'nt_28144',\n",
    "                     ]\n",
    "            )\n",
    "    .mark_point(filled=True,\n",
    "                size=50,\n",
    "                opacity=0.5)\n",
    "    .add_selection(outgroup_selection,\n",
    "                   aligner_selection)\n",
    "    .transform_filter(outgroup_selection)\n",
    "    .transform_filter(aligner_selection)\n",
    "    )\n",
    "\n",
    "deleted_delta_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbf540-dc1e-4e46-b613-15252075b653",
   "metadata": {},
   "source": [
    "For the rest of the analysis, we filter to just the samples with sufficient coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a43ba-b21b-43b4-a4d7-16174d287534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Just retaining samples with >={min_frac_coverage} coverage\")\n",
    "\n",
    "filtered_deleted_diffs = (\n",
    "    deleted_diffs\n",
    "    .query('frac_called_region >= @min_frac_coverage')\n",
    "    .assign(sample=lambda x: pd.Categorical(x['sample'], samples, ordered=True))\n",
    "    .sort_values('sample')\n",
    "    )\n",
    "\n",
    "filtered_deleted_diffs_display = (\n",
    "    filtered_deleted_diffs\n",
    "    [['sample', 'frac_called_region', 'patient_group', 'substitutions_str', 'n_substitutions']]\n",
    "    .drop_duplicates()\n",
    "    .assign(substitutions_str=lambda x: x['substitutions_str'].str.replace(',', ', '))\n",
    "    .rename(columns={'frac_called_region': f\"fraction sites called ({start}-{end})\", \n",
    "                     'patient_group': 'patient group',\n",
    "                     'n_substitutions': 'number of substitutions',\n",
    "                     'substitutions_str': f\"substitutions relative to {ref_genome_name}\"\n",
    "                     })\n",
    "    )\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "display(filtered_deleted_diffs_display)\n",
    "\n",
    "print(f\"Saving table to {deleted_diffs_latex}\")\n",
    "filtered_deleted_diffs_display.to_latex(deleted_diffs_latex,\n",
    "                                        float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11c275-0779-4d29-892e-178c8912f9ae",
   "metadata": {},
   "source": [
    "## Plot jitters of distance to outgroup\n",
    "First make data frame to plot that combines deleted data set and Wuhan early sequences (from January).\n",
    "Here we get the data for the deleted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceca393-7dec-4385-9648-4b9bad47bb40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(aligners) == 1, 'code below only works for one aligner, otherwise add aligner selection'\n",
    "\n",
    "deleted_jitter_df = (\n",
    "    filtered_deleted_diffs\n",
    "    .query('patient_group == \"early outpatient\"')\n",
    "    [['sample', 'patient_group', 'n_substitutions', 'substitutions',\n",
    "      'substitutions_to_outgroup', 'frac_called_region', 'outgroup',\n",
    "      'delta_distance_to_outgroup']]\n",
    "    .rename(columns={'sample': 'strain',\n",
    "                     'frac_called_region': 'frac_called',\n",
    "                     'patient_group': 'group'},\n",
    "            )\n",
    "    .assign(category='deleted PRJNA612766',\n",
    "            huanan_market=False,\n",
    "            date='early Wuhan epidemic')\n",
    "    )\n",
    "\n",
    "deleted_jitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc53d2-af98-4c75-8c36-fbd692c2bcb8",
   "metadata": {},
   "source": [
    "For the early sequence set, just get sequences in January:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647dfc9b-3e4d-403c-8b99-87754a1ce66d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_date_group(date):\n",
    "    if date <= pd.to_datetime('2020-01-15'):\n",
    "        return 'before Jan 15'\n",
    "    elif pd.to_datetime('2020-01-15') < date <= last_date:\n",
    "        assert last_date == pd.to_datetime('2020-01-31')\n",
    "        return 'Jan 15-31'\n",
    "    else:\n",
    "        return ValueError(f\"{date=} out of range\")\n",
    "    \n",
    "\n",
    "early_seqs_jitter_df = (\n",
    "    delta_dist_region\n",
    "    .query('date <= @last_date')\n",
    "    .sort_values(['location_category', 'date'])\n",
    "    .assign(date_group=lambda x: x['date'].map(assign_date_group),\n",
    "            group=lambda x: x['location_category'] + ' ' + x['date_group'],\n",
    "            category=lambda x: x['location_category'],\n",
    "            date=lambda x: x['date'].astype(str),\n",
    "            )\n",
    "    [['strain', 'group', 'n_substitutions', 'substitutions',\n",
    "      'substitutions_to_outgroup', 'frac_called', 'outgroup',\n",
    "      'delta_distance_to_outgroup', 'category', 'huanan_market',\n",
    "      'date']]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3db90b-c3ef-4c01-a875-5b7de6e5e5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jitter_df = pd.concat([deleted_jitter_df, early_seqs_jitter_df])\n",
    "\n",
    "groups = jitter_df['group'].unique()\n",
    "\n",
    "y_extent = jitter_df['delta_distance_to_outgroup'].max() - jitter_df['delta_distance_to_outgroup'].min() + 1\n",
    "jitter_df = (\n",
    "    jitter_df\n",
    "    .assign(jitter_x=lambda x: 0.21 * rand_jitter(len(x), seed=18),\n",
    "            jitter_y=lambda x: y_extent / 20 * rand_jitter(len(x), seed=1)\n",
    "            )\n",
    "    )\n",
    "\n",
    "dist_jitter_chart = (\n",
    "    alt.Chart(jitter_df)\n",
    "    .encode(column=alt.Column('group',\n",
    "                              title=None,\n",
    "                              header=alt.Header(labelAngle=-90,\n",
    "                                                labelOrient='bottom',\n",
    "                                                labelAlign='right',\n",
    "                                                labelPadding=3,\n",
    "                                                ),\n",
    "                              sort=groups,\n",
    "                              ),\n",
    "            x=alt.X('jitter_x:Q',\n",
    "                    title=None,\n",
    "                    axis=alt.Axis(values=[0], ticks=True, grid=False, labels=False),\n",
    "                    scale=alt.Scale(domain=[-1, 1]),\n",
    "                    ),\n",
    "            y=alt.Y('y:Q',\n",
    "                    title='relative mutations from outgroup',\n",
    "                    scale=alt.Scale(nice=False),\n",
    "                    axis=alt.Axis(tickMinStep=1),\n",
    "                    ),\n",
    "            color=alt.Color('category:N',\n",
    "                            sort=groups,\n",
    "                            scale=alt.Scale(range=['#56B4E9', '#E69F00',\n",
    "                                                   '#009E73', '#F0E442']),\n",
    "                            legend=alt.Legend(title='sequence set')\n",
    "                            ),\n",
    "            tooltip=['strain',\n",
    "                     'date',\n",
    "                     alt.Tooltip('n_substitutions',\n",
    "                                 title='number substitutions'),\n",
    "                     'substitutions',\n",
    "                     alt.Tooltip('substitutions_to_outgroup',\n",
    "                                 title='substitutions to outgroup'),\n",
    "                     alt.Tooltip('frac_called',\n",
    "                                 title='fraction sites called',\n",
    "                                 format='.3f'),\n",
    "                     alt.Tooltip('huanan_market',\n",
    "                                 title='from seafood market')\n",
    "                     ],\n",
    "            )\n",
    "    .mark_point(filled=True,\n",
    "                opacity=0.45,\n",
    "                size=40,\n",
    "                )\n",
    "    .add_selection(outgroup_selection,\n",
    "                   jitter_slider)\n",
    "    .transform_filter(outgroup_selection)\n",
    "    .transform_calculate(\n",
    "            y='datum.delta_distance_to_outgroup + datum.jitter_y * y_axis_jitter.amount'\n",
    "            )\n",
    "    .configure_axis(grid=False)\n",
    "    .configure_view(stroke=None)\n",
    "    .configure_facet(spacing=0)\n",
    "    .properties(height=175,\n",
    "                width=40)\n",
    "    )\n",
    "\n",
    "for f in deltadist_jitter_charts:\n",
    "    print(f\"Saving to {f}\")\n",
    "    if os.path.splitext(f) == '.html':\n",
    "        dist_jitter_chart.save(f)\n",
    "    else:\n",
    "        altair_saver.save(dist_jitter_chart, f)\n",
    "        altair_saver.save(dist_jitter_chart, '_temp.png')\n",
    "\n",
    "dist_jitter_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4ca3f-279f-4e0b-bdbe-d5b234dbcefe",
   "metadata": {},
   "source": [
    "## Write out alignments\n",
    "For each alignment, we only get sequences that have unique substitutions relative to the reference, and collapse within these sets.\n",
    "\n",
    "First get data frames with the relevant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d804bab4-a0e0-414c-bf7f-320920d49bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs_all_d = {s.id: str(s.seq).upper()[ignore_muts_before - 1: ignore_muts_after]\n",
    "              for s in Bio.SeqIO.parse(early_seq_alignment_file, 'fasta')}\n",
    "\n",
    "all_alignment_df = (\n",
    "    delta_dist\n",
    "    .query('date <= @last_date')\n",
    "    .assign(strain_date=lambda x: x['strain'] + ' (' + x['date'].astype(str) + ')')\n",
    "    [['strain', 'strain_date', 'substitutions', 'frac_called']]\n",
    "    .assign(sequence=lambda x: x['strain'].map(seqs_all_d))\n",
    "    .assign(site_offset=ignore_muts_before)\n",
    "    .query('frac_called >= @min_frac_called')\n",
    "    .drop_duplicates()\n",
    "    )\n",
    "\n",
    "seqs_region_d = {s.id: str(s.seq).upper()[start - 1: end]\n",
    "                 for s in Bio.SeqIO.parse(early_seq_alignment_file, 'fasta')}\n",
    "\n",
    "for tup in filtered_deleted_diffs.itertuples():\n",
    "    seqs_region_d['early_Wuhan_epidemic/' + tup.sample] = tup.sequence_region\n",
    "    \n",
    "region_alignment_df = (\n",
    "    delta_dist_region\n",
    "    .query('date <= @last_date')\n",
    "    .assign(strain_date=lambda x: x['strain'] + ' (' + x['date'].astype(str) + ')')\n",
    "    [['strain', 'strain_date', 'substitutions', 'frac_called']]\n",
    "    .append(deleted_jitter_df\n",
    "            [['strain', 'substitutions', 'frac_called']]\n",
    "            .assign(strain=lambda x: 'early_Wuhan_epidemic/' + x['strain'].astype(str),\n",
    "                    strain_date=lambda x: x['strain'])\n",
    "            )\n",
    "    .assign(sequence=lambda x: x['strain'].map(seqs_region_d))\n",
    "    .assign(site_offset=start)\n",
    "    .query('frac_called >= @min_frac_called')\n",
    "    .drop_duplicates()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463f799-a172-4f66-abc7-e2b2deedf569",
   "metadata": {},
   "source": [
    "Now remove any mutations in the list to ignore or that are rare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ca9e7-40e0-46a8-b71a-9bd60d5c629a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "muts_to_ignore = set(muts_to_ignore)\n",
    "print(f\"There are {len(muts_to_ignore)} mutations to ignore:\\n{muts_to_ignore}\")\n",
    "\n",
    "alignment_df = {'all': all_alignment_df.copy(),\n",
    "                'region': region_alignment_df.copy()}\n",
    "\n",
    "def filter_seq(seq, subs_to_remove, site_offset):\n",
    "    seq = list(seq)\n",
    "    for s in subs_to_remove:\n",
    "        wt, site, mut = s[0], int(s[1: -1]) - site_offset, s[-1]\n",
    "        assert seq[site] == mut, f\"{seq[site]=}, {wt=}, {s=}\"\n",
    "        seq[site] = wt\n",
    "    return ''.join(seq)\n",
    "\n",
    "for desc in ['all', 'region']:\n",
    "    singletons = {s for s, n in \n",
    "                  collections.Counter([s for s in\n",
    "                                       ','.join(alignment_df[desc]['substitutions']).split(',') if s]\n",
    "                                      ).items()\n",
    "                  if n <= collapse_rare_muts\n",
    "                  }\n",
    "    print(f\"For the {desc} set, there are {len(singletons)} mutations to collapse \"\n",
    "          f\"because they are found <= {collapse_rare_muts} times\")\n",
    "    alignment_df[desc] = (\n",
    "        alignment_df[desc]\n",
    "        .assign(substitutions_to_remove=lambda x: x['substitutions'].map(\n",
    "                        lambda subs: {s for s in subs.split(',')\n",
    "                                      if s in muts_to_ignore or s in singletons}\n",
    "                        ),\n",
    "                n_substitutions_removed=lambda x: x['substitutions_to_remove'].map(len),\n",
    "                substitutions=lambda x: x.apply(lambda r: ','.join([s for s in r['substitutions'].split(',')\n",
    "                                                                    if s not in r['substitutions_to_remove']]),\n",
    "                                                axis=1),\n",
    "                sequence=lambda x: x.apply(lambda r: filter_seq(r['sequence'],\n",
    "                                                                   r['substitutions_to_remove'],\n",
    "                                                                   r['site_offset']),\n",
    "                                           axis=1),\n",
    "                )\n",
    "        .drop(columns='substitutions_to_remove')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6421df-8d9f-4943-85fb-02b65f3425be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T01:25:18.888267Z",
     "iopub.status.busy": "2021-06-09T01:25:18.887777Z",
     "iopub.status.idle": "2021-06-09T01:25:18.900617Z",
     "shell.execute_reply": "2021-06-09T01:25:18.899739Z",
     "shell.execute_reply.started": "2021-06-09T01:25:18.888221Z"
    },
    "tags": []
   },
   "source": [
    "Now just get one representative sequence from each set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2e9f9-e8cb-41b3-8021-588806365558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for desc in ['all', 'region']:\n",
    "    print(f\"For {desc}, starting with {len(alignment_df[desc])} sequences\")\n",
    "    assert all(alignment_df[desc]['sequence'].map(len) ==\n",
    "               alignment_df[desc]['sequence'].map(len).values[0])\n",
    "    alignment_df[desc] = (\n",
    "        alignment_df[desc]\n",
    "        .sort_values(['n_substitutions_removed', 'frac_called'],\n",
    "                     ascending=[True, False])\n",
    "        .groupby('substitutions', as_index=False)\n",
    "        .aggregate(nstrains=pd.NamedAgg('strain', 'count'),\n",
    "                   representative_strain=pd.NamedAgg('strain', 'first'),\n",
    "                   sequence=pd.NamedAgg('sequence', 'first'),\n",
    "                   all_strains=pd.NamedAgg('strain', ', '.join),\n",
    "                   all_strains_dates=pd.NamedAgg('strain_date', ', '.join)\n",
    "                   )\n",
    "        )\n",
    "    print(f\"After collapsing, have {len(alignment_df[desc])} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75aaf3c-a9b3-412f-ac83-b96a39c636e9",
   "metadata": {},
   "source": [
    "Now filter out rare variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24434119-4ca1-4e91-ae04-764c87eb9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Filtering out variants observed <= {filter_rare_variants} times\")\n",
    "for desc in ['all', 'region']:\n",
    "    print(f\"For {desc}, starting with {len(alignment_df[desc])} sequences\")\n",
    "    alignment_df[desc] = (\n",
    "        alignment_df[desc]\n",
    "        .query('nstrains > @filter_rare_variants')\n",
    "        )\n",
    "    print(f\"After filtering, have {len(alignment_df[desc])} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8394c81-9533-46bc-8e80-c6a093fabb7b",
   "metadata": {},
   "source": [
    "Write the alignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50aa70-8a43-4377-b33d-9008135811b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for desc, alignment_file, alignment_csv in [\n",
    "        ('all', alignment_all_fasta, alignment_all_csv),\n",
    "        ('region', alignment_region_fasta, alignment_region_csv)\n",
    "        ]:\n",
    "    df = alignment_df[desc]\n",
    "    print(f\"Writing {len(df)} sequences to {alignment_file} and {alignment_csv}\")\n",
    "    df.drop(columns='sequence').to_csv(alignment_csv, index=False)\n",
    "    a = [Bio.SeqRecord.SeqRecord(seq=Bio.Seq.Seq(tup.sequence),\n",
    "                                 id=tup.representative_strain,\n",
    "                                 name='',\n",
    "                                 description='')\n",
    "         for tup in df.itertuples()]\n",
    "    Bio.SeqIO.write(a, alignment_file, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb5d40-a1f8-45ea-8b87-01395a39ffdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
